{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9d6899",
   "metadata": {},
   "source": [
    "# HSST B10m Data pre-processing with SVM basic examples\n",
    "### Introduction\n",
    "The below are worked examples from the slides to help support learning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de348d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce4a58",
   "metadata": {},
   "source": [
    "The below is a helper method to plot the SVM boundaries between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_plot(X,y,clf,x_axis_label=\"PC1\", y_axis_label=\"PC2\"):\n",
    "    \"\"\"\n",
    "    Based on answer provided by S. Loukas on StackOverflow.\n",
    "    https://stackoverflow.com/questions/51495819/how-to-plot-svm-decision-boundary-in-sklearn-python\n",
    "    \"\"\"\n",
    "    def make_meshgrid(x, y, h=.02):\n",
    "        x_min, x_max = x.min() - 1, x.max() + 1\n",
    "        y_min, y_max = y.min() - 1, y.max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        return xx, yy\n",
    "\n",
    "    def plot_contours(ax, clf, xx, yy, **params):\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        out = ax.contourf(xx, yy, Z, **params)\n",
    "        return out\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    # title for the plots\n",
    "    title = ('Decision surface of linear SVC ')\n",
    "\n",
    "    # Set-up grid for plotting.\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "    plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_ylabel(y_axis_label)\n",
    "    ax.set_xlabel(x_axis_label)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f27f9",
   "metadata": {},
   "source": [
    "### Body Temperature\n",
    "Simple body temp example, where we have a data that is both higher and lower than the normal range, so is not linearly separable. We standardise and then square to create a pseudo-dimension which transforms the problem into a linearly separable one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate equal amounts of data:   34-35.8C (n = 20), 36-38 (n=40), 38.2-40 (n=20)\n",
    "t = np.r_[np.random.randint(340,358,20),np.random.randint(360,380,40), np.random.randint(382,400,20)]/10\n",
    "# label the data, 0 for ok, and 1 for outside healthy range.\n",
    "y = np.zeros(80)\n",
    "y = np.where(t<36,1,y)\n",
    "y = np.where(t>38,1,y)\n",
    "\n",
    "# manual standardise\n",
    "z=(t-np.mean(t))/np.std(t,ddof=1)\n",
    "\n",
    "# add pseudo-dimension as a \"Kernel trick\"\n",
    "X=np.c_[z, z**2]\n",
    "\n",
    "# fit an SVM and plot, we set the regularisation parameter to 1\n",
    "model = SVC(kernel='linear', C=100)\n",
    "clf = model.fit(X,y)\n",
    "# plot to show the classification.\n",
    "decision_plot(X,y,clf,r\"$Temp_{std}$\",r\"$Temp^2_{std}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650b810",
   "metadata": {},
   "source": [
    "### Wine Dataset\n",
    "Here we use the wine dataset which we worked through in the slides and complete the example by using an SVM to split the data into 3 classes. The SVM is naturally a binary classify, so to make it multiclass we need to do either one-vs-one with the classes or one-vs-rest, the sckit-learn implementation is one-vs-one.\n",
    "\n",
    "This is based on:\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf82e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load data and plit into test/train samples\n",
    "\n",
    "X, y = load_wine(return_X_y=True, as_frame=True)\n",
    "target_classes = range(0, 3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "for standardise in [False, True]:\n",
    "    # Optionally standardise data, we iterate over both to compare.\n",
    "    if standardise:\n",
    "        print(\"Example ran with Standardisation\")\n",
    "        scaler = StandardScaler()#.set_output(transform=\"pandas\")\n",
    "        scaled_X_train = scaler.fit_transform(X_train)\n",
    "        scaled_X_test = scaler.transform(X_test)\n",
    "    else: \n",
    "        print(\"Example ran without Standardisation\")\n",
    "        scaled_X_train = X_train\n",
    "        scaled_X_test = X_test\n",
    "    \n",
    "    # perform PCA, we use 2 so that we can plot readily.\n",
    "    print(\"PCA\")\n",
    "    n_components = 2\n",
    "    pca = PCA(n_components=n_components, svd_solver=\"randomized\", whiten=True).fit(scaled_X_train)  \n",
    "    X_train_pca = pca.transform(scaled_X_train)\n",
    "    X_test_pca = pca.transform(scaled_X_test)\n",
    "    \n",
    "    # fit a linear SVM, note we do a randomize search on C between 1 and 100.\n",
    "    # if you wish to observe rbf/poly then you need to include the gamma with sensible ranges.\n",
    "    param_grid = {\n",
    "        \"C\": loguniform(1e0, 1e2),\n",
    "        #\"gamma\": loguniform(1e-4, 1e0),\n",
    "    }\n",
    "    clf = RandomizedSearchCV(\n",
    "        SVC(kernel=\"linear\", class_weight=\"balanced\"), param_grid, n_iter=10\n",
    "    )\n",
    "    clf = clf.fit(X_train_pca, y_train)\n",
    "    \n",
    "    # print out the coefs for the best fit\n",
    "    print(\"Best estimator found by grid search:\")\n",
    "    print(clf.best_estimator_)\n",
    "    \n",
    "    # plot the boundaries between classes in PC1 and PC2 space.\n",
    "    decision_plot(X_train_pca, y_train,clf)\n",
    "    y_pred = clf.predict(X_train_pca)\n",
    "    \n",
    "    # run classification reports on the training and test data\n",
    "    print(\"Training data\")\n",
    "    print(classification_report(y_train, y_pred, target_names=np.char.mod(\"Class %d\",target_classes)))\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    print(\"Test data\")\n",
    "    print(classification_report(y_test, y_pred, target_names=np.char.mod(\"Class %d\",target_classes)))\n",
    "    print(\"-------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (2023)",
   "language": "python",
   "name": "python3-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
